import os
import pickle

from selenium import webdriver
from selenium.webdriver.common.by import By

from tqdm import tqdm
from typing import Tuple

# pip install openpyxl
from openpyxl import Workbook
from openpyxl.worksheet.worksheet import Worksheet


# ğŸš€ ì½”ë””ë“¤ì„ ë½‘ì•„ì˜¬ í˜ì´ì§€ ì§€ì •
COORDI_LIST_PATH = "https://www.musinsa.com/app/styles/lists?sort=view_cnt&page="
COORDI_BASE_PATH = "https://www.musinsa.com/app/styles/views/"

# ğŸš€ (ì½”ë”” ë§í¬, ìŠ¤íƒ€ì¼ ì •ë³´, ì½”ë”” ì´ë¯¸ì§€ url) ë¦¬ìŠ¤íŠ¸ ë°›ì•„ì˜¤ê¸°
def get_codi_info(driver: webdriver.Chrome) -> Tuple[list, list, list]:
    
    codi_link_result = driver.find_elements(by=By.CSS_SELECTOR, value=".style-list-item__thumbnail > a")
    style_result = driver.find_elements(by=By.CSS_SELECTOR, value=".style-list-information__text")
    image_url_result = driver.find_elements(by=By.CSS_SELECTOR, value=".style-list-item__thumbnail > a > div.style-list-thumbnail > img")
    codi_popularity = driver.find_elements(by=By.CSS_SELECTOR, value='.post-information > .post-information__text')

    print (f"""
           # of codi_links : {len(codi_link_result)}
           # of style : {len(style_result)}
           # of img_urls : {len(image_url_result)}
           """)

    codi_id_list = list()
    codi_style_list = list()
    codi_url_list = list()
    pop_list = list()

    # â­ 1. ì½”ë”” ID ì •ë³´ javascript function ì¸ìë¡œ ë¶€í„° ë°›ì•„ì˜¤ê¸°
    for link_element in codi_link_result:
        js_function = link_element.get_attribute("onclick")
        codi_id = js_function.split("'")[1]
        codi_id_list.append(codi_id)
        
    # â­ 2. style ì •ë³´ textë¡œ ì¶”ì¶œí•˜ê¸°
    for style_element in style_result:
        codi_style_list.append(style_element.text)
        
    # â­ 3. ì½”ë””ì˜ ì´ë¯¸ì§€ url ë°›ì•„ì˜¤ê¸°
    for img_element in image_url_result:
        codi_url_list.append(img_element.get_attribute('src'))

    already_crawled_codi = list()
    with open("/opt/ml/input/data/already/codi.pickle", "rb") as f:
        try:
            already_crawled_codi = pickle.load(f)
        except:
            pass

    with open("/opt/ml/input/data/already/codi.pickle", "wb") as f:
        already_crawled_codi.extend(codi_url_list)
        pickle.dump(already_crawled_codi, f)

    # 4. ì½”ë””ì˜ ì¡°íšŒìˆ˜ ë°›ì•„ì˜¤ê¸°
    for popularity_element in codi_popularity:
        if "ì¡°íšŒ" in popularity_element.text:
            cnt = int(popularity_element.text[3:].replace(",", ""))
            pop_list.append(cnt)
        
    return codi_id_list, codi_style_list, codi_url_list, pop_list

"""
1. í˜„ì¬ ì½”ë””ì— ì–´ë–¤ ì•„ì´í…œë“¤ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
2. ì½”ë”” ì •ë³´ ìˆ˜ì§‘
3. ì–´ë–¤ ì½”ë””ë“¤ì„ í¬ë¡¤ë§ í–ˆëŠ”ì§€ ì €ì¥
"""


# ğŸš€ ì½”ë”” ë§í¬ì— í•˜ë‚˜ì”© ì ‘ì†í•˜ë©´ì„œ, ì—°ê´€ëœ ìƒí’ˆ ID, ì½”ë””íƒœê·¸ ë°›ì•„ì˜¤ê¸°
def make_crawl_xlsx(driver: webdriver.Chrome, sheets: Tuple[Worksheet, Worksheet, Worksheet]):
    
    sheet_codi = sheets[0]
    sheet_codi_tag = sheets[1]
    sheet_item_codi_id = sheets[2]
    
    # (ì½”ë”” ë§í¬, ìŠ¤íƒ€ì¼ ì •ë³´, ì½”ë”” ì´ë¯¸ì§€ url) ë¦¬ìŠ¤íŠ¸ ë°›ì•„ì˜¤ê¸°
    codi_id_list, codi_style_list, codi_url_list, pop_list = get_codi_info(driver)
    
    for codi_id, codi_style, codi_img_url, popularity in tqdm(zip(codi_id_list, codi_style_list, codi_url_list, pop_list), total=len(codi_id_list), desc="Codi crawling progress"):
            
        # í•˜ë‚˜ì˜ ì½”ë””ì— ëŒ€í•œ information ì €ì¥
        codi_info = list()
        
        # â­ 4. ì½”ë””ì˜ ìƒì„¸ì •ë³´ê°€ ìˆëŠ” url ë°›ì•„ì˜¤ê¸°
        codi_path = COORDI_BASE_PATH + codi_id
        
        # ì½”ë””ì˜ ê²½ë¡œ ë°›ì•„ì˜¤ê³  ì½”ë”” ìƒì„¸ì •ë³´ í˜ì´ì§€ ì§„ì…í•˜ê¸°
        driver.get(codi_path)
        
        # â­ 6. í˜„ì¬ ì½”ë””ì— í¬í•¨ëœ ì•„ì´í…œ id ë°›ì•„ì˜¤ê¸° (ë‹¤ë¥¸ sheetì— ì €ì¥)
        item_elements = driver.find_elements(by=By.CSS_SELECTOR, value=".styling_img")
        if len(item_elements) <= 1:
            print ("í˜„ì¬ ì½”ë””ì— ì¡´ì¬í•˜ëŠ” ì•„ì´í…œì˜ ìˆ˜ê°€ 1ê°œ ë¯¸ë§Œì…ë‹ˆë‹¤.")
            print ("ë”°ë¼ì„œ, í˜„ì¬ ì½”ë””ì— ì¡´ì¬í•˜ëŠ” ì•„ì´í…œì˜ í¬ë¡¤ë§ì€ ì§„í–‰í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            continue

        for item_element in item_elements:
            item_id = item_element.get_attribute("href").split("/")[-2]
            print (f"Connecting  Item #{item_id} ----- Codi #{codi_id}")
            sheet_item_codi_id.append([item_id, codi_id])

        # â­ 5. ì½”ë”” íƒœê·¸ ë°›ì•„ì˜¤ê¸° (ë‹¤ë¥¸ sheetì— ì €ì¥)
        coordi_tags = driver.find_elements(by=By.CSS_SELECTOR, value=".ui-tag-list")
        for tag_element in coordi_tags:
            sheet_codi_tag.append([codi_id, tag_element.text])
        
            
        codi_info.append(codi_id)
        codi_info.append(codi_style)
        codi_info.append(codi_img_url)
        codi_info.append(codi_path)
        codi_info.append(popularity)
        sheet_codi.append(codi_info)
        
        
# ğŸš€ í¬ë¡¤ë§ ì™„ë£Œëœ íŒŒì¼ ì €ì¥
def save_as_xlsx(workbooks: Tuple[Workbook, Workbook, Workbook]):
    wb_codi = workbooks[0]
    wb_codi_tag = workbooks[1]
    wb_item_codi_id = workbooks[2]
        
    subpath = 'recent'
    if "sort" in COORDI_LIST_PATH:
        subpath = 'view'
    
    PATH = '/opt/ml/input/data/raw_codishop/' + subpath + '/codi/'
    ITEM_PATH = '/opt/ml/input/data/raw_codishop/' + subpath + '/item/'
    os.makedirs(PATH, exist_ok=True)
    wb_codi.save(os.path.join(PATH, "codi.xlsx"))
    wb_codi_tag.save(os.path.join(PATH, "codi_tag.xlsx"))
    wb_item_codi_id.save(os.path.join(ITEM_PATH, "item_codi_id.xlsx"))
    
# ğŸš€ ìµœìƒìœ„ ë©”ì¸ í˜ì´ì§€ ë¶ˆëŸ¬ì˜¤ê¸° (ì½”ë”” ëª©ë¡ 60ê°œ ë³´ì—¬ì§€ëŠ” í˜ì´ì§€)
def do_crawling(
    workbooks: Tuple[Workbook, Workbook, Workbook],
    sheets: Tuple[Worksheet, Worksheet, Worksheet],
    num_crawl_pages: int = 5,
    ):
    
    # ğŸš€ Chrome option ì„¤ì •
    chrome_options = webdriver.ChromeOptions()
    chrome_options.add_argument(argument='--headless')
    chrome_options.add_argument(argument='--no-sandbox')
    chrome_options.add_argument(argument='--disable-dev-shm-usage')
    
    driver = webdriver.Chrome(executable_path='chromedriver', options=chrome_options)
    driver.implicitly_wait(1.5)

    if "sort" in COORDI_LIST_PATH:
        print ("codi sort by 'ì¡°íšŒìˆœ'")
    else:
        print ("codi sort by 'ìµœì‹ ìˆœ'")
    
    for page_idx in range(1, num_crawl_pages + 1):
        print (f"Crawling {page_idx} pages..\nurl={COORDI_LIST_PATH + str(page_idx)}")
        driver.get(COORDI_LIST_PATH + str(page_idx))
        
        # "ë‚¨ì„±"ìœ¼ë¡œ ì„±ë³„ ê³ ì •
        button_male = driver.find_element(By.CSS_SELECTOR, "button.global-filter__button--mensinsa")
        button_male.click()
                
        make_crawl_xlsx(driver, sheets)
    
    driver.close()
    
    save_as_xlsx(workbooks)
    print ("í¬ë¡¤ë§ì´ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ëª¨ë“  íŒŒì¼ë“¤ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")