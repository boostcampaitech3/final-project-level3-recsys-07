import openpyxl
import pickle
import pandas as pd

from selenium import webdriver
from selenium.webdriver.common.by import By
from easydict import EasyDict
from utils import *

#-----------------------------------------
# ğŸŒŸ ê¼­ ì„¤ì •í•´ì•¼ í•˜ëŠ” íŒŒë¼ë¯¸í„°!
_VERBOSE = False

_SORT_OPTION = 'view'
# _SORT_OPTION = 'recent'

_STORE_OPTION = 'raw_codishop'
# _STORE_OPTION = 'raw_codimap'

# ì•„ì´í…œ í¬ë¡¤ë§ ì§„í–‰ ë²”ìœ„ ì„¤ì •
START_CODI_NUM = 0
END_CODI_NUM = 0
#-----------------------------------------

URL_PATH = None
if _SORT_OPTION == 'view':
    if _STORE_OPTION == 'raw_codishop':
        print ("ì½”ë””ìˆì—ì„œ ì¡°íšŒìˆœìœ¼ë¡œ ì •ë ¬")
        URL_PATH = "https://www.musinsa.com/app/styles/lists?sort=view_cnt"
    else:
        print ("ì½”ë””ë§µì—ì„œ ì¡°íšŒìˆœìœ¼ë¡œ ì •ë ¬")
        URL_PATH = "https://www.musinsa.com/app/codimap/lists?sort=view_cnt"
else:
    if _STORE_OPTION == 'raw_codishop':
        print ("ì½”ë””ìˆì—ì„œ ìµœì‹ ìˆœìœ¼ë¡œ ì •ë ¬")
        URL_PATH = "https://www.musinsa.com/app/styles/lists"
    else:
        print ("ì½”ë””ë§µì—ì„œ ì¡°íšŒìˆœìœ¼ë¡œ ì •ë ¬")
        URL_PATH = "https://www.musinsa.com/app/codimap/lists"
        


# ğŸš€ í¬ë¡¤ëŸ¬ ì˜µì…˜ ì„¤ì •
chrome_options = webdriver.ChromeOptions()
chrome_options.add_argument(argument='--headless') 
chrome_options.add_argument(argument='--no-sandbox')
chrome_options.add_argument(argument='--disable-dev-shm-usage')

# ğŸš€ í¬ë¡¤ëŸ¬ ì§€ì •
driver = webdriver.Chrome('chromedriver', options=chrome_options)
driver.implicitly_wait(3) #í˜ì´ì§€ë¥¼ ë¡œë”©í•˜ëŠ” ì‹œê°„ë™ì•ˆ ëŒ€ê¸°
driver.get(URL_PATH)

# ğŸš€ í¬ë¡¤ë§ ì™„ë£Œëœ ì •ë³´ë¥¼ ì €ì¥í•  excel sheet_codi ì§€ì •
workbooks = make_workbooks()
sheets = make_worksheets(workbooks)

# ğŸš€ ë‚¨ì„± ì½”ë””ë§Œ í¬ë¡¤ë§ í•˜ê¸° ìœ„í•´ì„œ ë²„íŠ¼ í´ë¦­
button = driver.find_element(By.CSS_SELECTOR, "button.global-filter__button--mensinsa")
button.click()

# ğŸš€ ì½”ë”” ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ url ë°›ì•„ì˜¤ê¸°
codi_info = pd.read_excel('/opt/ml/input/data/' + _STORE_OPTION + '/' + _SORT_OPTION + '/codi/codi.xlsx', engine='openpyxl')
# codi_info = codi_info.iloc[START_CODI_NUM : END_CODI_NUM]
codi_urls = codi_info["url"].to_list()
codi_ids = codi_info["id"].to_list()

# ğŸš€ ê° ì½”ë””ì— ëŒ€í•œ í¬ë¡¤ë§ ì§„í–‰
cnt = 0
seen_list = list()
crawled_codi_list = list()

for codi_id, codi_url in zip(codi_ids, codi_urls) :
    print(f"ì½”ë””ì— ì¡´ì¬í•˜ëŠ” ì•„ì´í…œ í¬ë¡¤ë§ CODI URL : {codi_url}")
    print(f"{cnt} out of {len(codi_urls)} codi crawled...")

    # ì½”ë””ì— í•˜ë‚˜ì”© ì ‘ê·¼
    try :
        driver.get(codi_url)
    except :
        print("ì´ ì—ëŸ¬ê°€ ë°œìƒí•˜ë©´ ë‹¤ìŒ ì½”ë””ë¶€í„° ë”°ë¡œ í¬ë¡¤ë§ í•´ì£¼ì‹œê¸¸ ë°”ëë‹ˆë‹¤!", flush=True)
        continue
    
    crawled_codi_list.append(str(codi_id))
    # ì½”ë”” ì•ˆì— ìˆëŠ” ì•„ì´í…œì— ëŒ€í•œ element ë°›ì•„ì˜¤ê¸°
    item_list = driver.find_elements(By.CSS_SELECTOR, 'div.styling_list > div.swiper-slide')
    item_urls = []
    
    if len(item_list) <= 1:
        print ("ì½”ë”” ë‚´ì— ì¡´ì¬í•˜ëŠ” ì•„ì´í…œì˜ ìˆ˜ê°€ 1ê°œ ì´í•˜ì´ë¯€ë¡œ í¬ë¡¤ë§ì„ ì§„í–‰í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        continue

    # ê° ì•„ì´í…œë“¤ì˜ url ì¶”ì¶œ
    for item in item_list:

        item_url = item.find_element(By.CSS_SELECTOR, "a.brand_item").get_attribute('href')

        # ì´ë¯¸ í¬ë¡¤ë§ ì§„í–‰í•œ itemì€ pass
        if item_url in seen_list:
            print ("í˜„ì¬ ì•„ì´í…œì€ ì´ë¯¸ í¬ë¡¤ë§ì´ ì™„ë£Œëœ ìƒíƒœì´ë¯€ë¡œ ê±´ë„ˆëœë‹ˆë‹¤.")
            continue

        seen_list.append(item_url)
        item_urls.append(item_url)

    # ê° ì•„ì´í…œë“¤ì„ ìˆœíšŒí•˜ë©´ì„œ í¬ë¡¤ë§ ì§„í–‰
    for item_url in item_urls:
        try : 
            driver.get(item_url)
        except :
            print (f"Failed to load item (item_url = {item_url})", flush=True)
            continue
        
        print(f"Crawling item : {item_url}")
        item_info = EasyDict()
        item_info.item_url = item_url
        item_info.codi_id  = codi_id


        item_info.id            = get_item_id(item_url)
        item_info.name          = get_item_name(driver)

        category                = driver.find_elements(By.CSS_SELECTOR, "p.item_categories > a")
        item_info.big_class     = get_big_class(category)
        item_info.mid_class     = get_mid_class(category)

        product_info                = driver.find_elements(By.CSS_SELECTOR, "ul.product_article > li > p.product_article_contents > strong")
        item_info.brand             = get_brand(product_info)
        item_info.serial_number     = get_serial_number(product_info)
        item_info.season            = get_season(driver)
        item_info.gender            = get_gender(driver)
        item_info.view_count        = get_view(driver)
        item_info.cum_sale          = get_cum_sale(driver)
        item_info.likes             = get_likes(driver)
        item_info.rating            = get_rating(driver)  
        item_info.price             = get_price(driver)
        item_info.img_url           = get_img_url(driver)
        item_info.tags_list         = get_tags_list(driver)
        item_info.buy_age_list      = get_buy_age_list(driver)
        item_info.buy_gender_list   = get_buy_gender_list(driver)
        item_info.rel_codi_url_list = get_rel_codi_url_list(driver, item_info.id, crawled_codi_list)  
        item_info.four_season_list, item_info.fit_list = get_fs_and_fit(driver)    
        
        # ìœ„ì—ì„œ í¬ë¡¤ë§í•œ ì •ë³´ë¥¼ sheetì— append
        save_to_sheets(sheets, item_info)

        # í˜„ì¬ ì•„ì´í…œ crawling ê²°ê³¼ ì¶œë ¥
        if _VERBOSE:
            print_crawled_item_info(item_info)

    cnt += 1

    # í¬ë¡¤ë§ ê²°ê³¼ íŒŒì¼ë¡œ ì €ì¥
    save_workbooks(workbooks, _SORT_OPTION, _STORE_OPTION)

driver.close()

with open("../pickles/item.pickle", "wb") as f:
    pickle.dump(seen_list, f)
