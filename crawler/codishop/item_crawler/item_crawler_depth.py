import openpyxl
import pickle
import pandas as pd

from selenium import webdriver
from selenium.webdriver.common.by import By
from easydict import EasyDict
from utils_depth import *

#-----------------------------------------
# ğŸŒŸ ê¼­ ì„¤ì •í•´ì•¼ í•˜ëŠ” íŒŒë¼ë¯¸í„°!
_VERBOSE = False

_SORT_OPTION = 'view'
# _SORT_OPTION = 'recent'

_STORE_OPTION = 'raw_codishop'
# _STORE_OPTION = 'raw_codimap'

# ì•„ì´í…œ í¬ë¡¤ë§ ì§„í–‰ ë²”ìœ„ ì„¤ì •
START_CODI_NUM = 0
END_CODI_NUM = 0
#-----------------------------------------

URL_PATH = None
if _SORT_OPTION == 'view':
    if _STORE_OPTION == 'raw_codishop':
        print ("ì½”ë””ìˆì—ì„œ ì¡°íšŒìˆœìœ¼ë¡œ ì •ë ¬")
        URL_PATH = "https://www.musinsa.com/app/styles/lists?sort=view_cnt"
    else:
        print ("ì½”ë””ë§µì—ì„œ ì¡°íšŒìˆœìœ¼ë¡œ ì •ë ¬")
        URL_PATH = "https://www.musinsa.com/app/codimap/lists?sort=view_cnt"
else:
    if _STORE_OPTION == 'raw_codishop':
        print ("ì½”ë””ìˆì—ì„œ ìµœì‹ ìˆœìœ¼ë¡œ ì •ë ¬")
        URL_PATH = "https://www.musinsa.com/app/styles/lists"
    else:
        print ("ì½”ë””ë§µì—ì„œ ì¡°íšŒìˆœìœ¼ë¡œ ì •ë ¬")
        URL_PATH = "https://www.musinsa.com/app/codimap/lists"





#### ì½”ë”” - ì•„ì´í…œ ë¦¬ìŠ¤íŠ¸ ####
already_worksheet = openpyxl.load_workbook("/opt/ml/input/data/raw_codishop/view/item/item_codi_id.xlsx").active
already_codi_item_list = list()
for item_id, codi_id in zip(already_worksheet['A'], already_worksheet['B']):
    already_codi_item_list.append((item_id, codi_id))
already_codi_item_list = already_codi_item_list[1:]
print (f"í˜„ì¬ ë³´ìœ í•œ ì—°ê²°ì •ë³´ : {len(already_codi_item_list)} ê°œ")


already_crawled_codi = list()
already_crawled_item = list()

with open("/opt/ml/input/data/already/codi.pickle", "rb") as f:
    already_crawled_codi = pickle.load(f)

with open("/opt/ml/input/data/already/item.pickle", "rb") as f:
    try:
        already_crawled_item = pickle.load(f)
    except:
        pass
    
# ğŸš€ í¬ë¡¤ëŸ¬ ì˜µì…˜ ì„¤ì •
chrome_options = webdriver.ChromeOptions()
chrome_options.add_argument(argument='--headless') 
chrome_options.add_argument(argument='--no-sandbox')
chrome_options.add_argument(argument='--disable-dev-shm-usage')

# ğŸš€ í¬ë¡¤ëŸ¬ ì§€ì •
driver = webdriver.Chrome('chromedriver', options=chrome_options)
driver.implicitly_wait(3) #í˜ì´ì§€ë¥¼ ë¡œë”©í•˜ëŠ” ì‹œê°„ë™ì•ˆ ëŒ€ê¸°
driver.get(URL_PATH)

# ğŸš€ í¬ë¡¤ë§ ì™„ë£Œëœ ì •ë³´ë¥¼ ì €ì¥í•  excel sheet_codi ì§€ì •
workbooks = make_workbooks()
sheets = make_worksheets(workbooks)

# ğŸš€ ë‚¨ì„± ì½”ë””ë§Œ í¬ë¡¤ë§ í•˜ê¸° ìœ„í•´ì„œ ë²„íŠ¼ í´ë¦­
button = driver.find_element(By.CSS_SELECTOR, "button.global-filter__button--mensinsa")
button.click()

# ğŸš€ ì½”ë”” ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ url ë°›ì•„ì˜¤ê¸°
codi_info = pd.read_excel('/opt/ml/input/data/raw_codishop/view/item/item_rel_codi_url.xlsx', engine='openpyxl')
# codi_info = codi_info.iloc[START_CODI_NUM : END_CODI_NUM]

codi_urls = codi_info["rel_codi_url"].to_list()

# ğŸš€ ê° ì½”ë””ì— ëŒ€í•œ í¬ë¡¤ë§ ì§„í–‰
cnt = 0
for codi_url in codi_urls :
    print(f"\nCrawling for CODI URL : {codi_url}")
    print(f"{cnt} out of {len(codi_urls)} codi crawled...")

    codi_id = codi_url.split("/")[-1]
    cnt += 1
    # ì½”ë””ì— í•˜ë‚˜ì”© ì ‘ê·¼
    try :
        driver.get(codi_url)
    except :
        print("ì´ ì—ëŸ¬ê°€ ë°œìƒí•˜ë©´ ë‹¤ìŒ ì½”ë””ë¶€í„° ë”°ë¡œ í¬ë¡¤ë§ í•´ì£¼ì‹œê¸¸ ë°”ëë‹ˆë‹¤!", flush=True)
        continue
    
    # ì½”ë”” ì•ˆì— ìˆëŠ” ì•„ì´í…œì— ëŒ€í•œ element ë°›ì•„ì˜¤ê¸°
    item_list = driver.find_elements(By.CSS_SELECTOR, 'div.styling_list > div.swiper-slide')
    item_urls = []

    if len(item_list) <= 1:
        print ("í˜„ì¬ ì½”ë””ì—ëŠ” 1ê°œ ë¯¸ë§Œì˜ ì•„ì´í…œì´ ì¡´ì¬í•˜ê¸° ë•Œë¬¸ì— í¬ë¡¤ë§ì„ ì§„í–‰í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.", flush=True)

    if codi_url in already_crawled_codi:
        print ("[item_crawler_depth.py] ì´ ì½”ë””ëŠ” ì´ë¯¸ í¬ë¡¤ë§ ëœì ì´ ìˆìŠµë‹ˆë‹¤.", flush=True)
        continue

    already_crawled_codi.append(str(codi_url))
    
    # ê° ì•„ì´í…œë“¤ì˜ url ì¶”ì¶œ
    for item in item_list:
        item_url = item.find_element(By.CSS_SELECTOR, "a.brand_item").get_attribute('href')
        item_urls.append(item_url)

    # ê° ì•„ì´í…œë“¤ì„ ìˆœíšŒí•˜ë©´ì„œ í¬ë¡¤ë§ ì§„í–‰
    for item_url in item_urls:
        try : 
            driver.get(item_url)
        except :
            print (f"Failed to load item (item_url = {item_url})", flush=True)
            continue
        
        item_id = get_item_id(item_url)
        if (item_id, codi_id) in already_codi_item_list:
            print (f"\n[item_crawler_depth.py] ì½”ë”” #{codi_id} ----- ì•„ì´í…œ #{item_id} ì˜ ì—°ê²°ì •ë³´ê°€ ì´ë¯¸ ìˆìŠµë‹ˆë‹¤.")
        else:
            print (f"\n[item_crawler_depth.py] ì½”ë”” #{codi_id} --X-- ì•„ì´í…œ #{item_id} ì˜ ì—°ê²°ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
            already_codi_item_list.append((item_id, codi_id))


        print(f"ì•„ì´í…œ #{item_id} ì— ëŒ€í•œ í¬ë¡¤ë§ì„ ì§„í–‰í•©ë‹ˆë‹¤!! ì›¹ URL : {item_url}")
        if item_url in already_crawled_item:
            print (f"í˜„ì¬ ì•„ì´í…œ #{item_id} ëŠ” ì´ë¯¸ í¬ë¡¤ë§ì´ ì™„ë£Œëœ ìƒíƒœì´ë¯€ë¡œ ê±´ë„ˆëœë‹ˆë‹¤.")
            continue

        already_crawled_item.append(item_url)

        item_info = EasyDict()
        item_info.item_url = item_url
        item_info.codi_id  = codi_id

        item_info.id            = item_id
        item_info.name          = get_item_name(driver)

        category      = driver.find_elements(By.CSS_SELECTOR, "p.item_categories > a")
        item_info.big_class     = get_big_class(category)
        item_info.mid_class     = get_mid_class(category)

        product_info  = driver.find_elements(By.CSS_SELECTOR, "ul.product_article > li > p.product_article_contents > strong")
        item_info.brand         = get_brand(product_info)
        item_info.serial_number = get_serial_number(product_info)
        item_info.season        = get_season(driver)
        item_info.gender        = get_gender(driver)
        item_info.view_count    = get_view(driver)
        item_info.cum_sale      = get_cum_sale(driver)
        item_info.likes         = get_likes(driver)
        item_info.rating        = get_rating(driver)  
        item_info.price         = get_price(driver)
        item_info.img_url       = get_img_url(driver)
        
        #-- ì£¼ì˜: í¬ë¡¤ë§ì˜ ì¼ê´€ì„±ì´ ë†’ì§€ ì•ŠìŒ
        try: menu = driver.find_elements(By.CSS_SELECTOR, "div#goods_opt_area > select")
        except: menu = None

        item_info.tags_list       = get_tags_list(driver)
        item_info.buy_age_list    = get_buy_age_list(driver)
        item_info.buy_gender_list = get_buy_gender_list(driver)
        item_info.four_season_list, item_info.fit_list = get_fs_and_fit(driver)    
        item_info.rel_codi_url_list = get_rel_codi_url_list(driver, item_info.id, already_crawled_codi)  
        
        # ìœ„ì—ì„œ í¬ë¡¤ë§í•œ ì •ë³´ë¥¼ sheetì— append
        save_to_sheets(sheets, item_info)

        # í¬ë¡¤ë§ ê²°ê³¼ íŒŒì¼ë¡œ ì €ì¥
        # save_workbooks(workbooks, _SORT_OPTION, _STORE_OPTION)

        # í˜„ì¬ ì•„ì´í…œ crawling ê²°ê³¼ ì¶œë ¥
        if _VERBOSE:
            print_crawled_item_info(item_info)

    save_workbooks(workbooks, _SORT_OPTION, _STORE_OPTION)
driver.close()


with open("/opt/ml/input/data/already/codi.pickle", "wb") as f:
    pickle.dump(already_crawled_codi, f)

with open("/opt/ml/input/data/already/item.pickle", "wb") as f:
    pickle.dump(already_crawled_item, f)

already_worksheet = openpyxl.Workbook().active
already_worksheet.append(['id', 'codi_id'])
for (item_id, codi_id) in already_codi_item_list:
    already_worksheet.append([item_id, codi_id])
already_worksheet.save('/opt/ml/input/data/raw_codishop/view/item/item_codi_id.xlsx')

