import pandas as pd

from selenium import webdriver
from selenium.webdriver.common.by import By
from easydict import EasyDict
from utils import *

# ğŸŒŸ ê¼­ ì„¤ì •í•´ì•¼ í•˜ëŠ” íŒŒë¼ë¯¸í„°!
_VERBOSE = True
URL_PATH = "https://www.musinsa.com/app/styles/lists"

# ğŸš€ í¬ë¡¤ëŸ¬ ì˜µì…˜ ì„¤ì •
chrome_options = webdriver.ChromeOptions()
chrome_options.add_argument(argument='--headless') 
chrome_options.add_argument(argument='--no-sandbox')
chrome_options.add_argument(argument='--disable-dev-shm-usage')

# ğŸš€ í¬ë¡¤ëŸ¬ ì§€ì •
driver = webdriver.Chrome('chromedriver', options=chrome_options)
driver.get(URL_PATH)
driver.implicitly_wait(3) #í˜ì´ì§€ë¥¼ ë¡œë”©í•˜ëŠ” ì‹œê°„ë™ì•ˆ ëŒ€ê¸°

# ğŸš€ í¬ë¡¤ë§ ì™„ë£Œëœ ì •ë³´ë¥¼ ì €ì¥í•  excel sheet_codi ì§€ì •
workbooks = make_workbooks()
sheets = make_worksheets(workbooks)

# ğŸš€ ë‚¨ì„± ì½”ë””ë§Œ í¬ë¡¤ë§ í•˜ê¸° ìœ„í•´ì„œ ë²„íŠ¼ í´ë¦­
button = driver.find_element(By.CSS_SELECTOR, "button.global-filter__button--mensinsa")
button.click()

# ğŸš€ ì½”ë”” ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ url ë°›ì•„ì˜¤ê¸°
codi_info = pd.read_excel("../codi_crawler/asset/codi.xlsx")
codi_urls = codi_info["url"].to_list()
codi_ids = codi_info["id"].to_list()

# ğŸš€ ê° ì½”ë””ì— ëŒ€í•œ í¬ë¡¤ë§ ì§„í–‰
cnt = 0
for codi_id, codi_url in zip(codi_ids, codi_urls) :
    print(f"Crawling for CODI URL : {codi_url}\n")
    print(f"{cnt} out of {len(codi_urls)} codi crawled...")

    # ì½”ë””ì— í•˜ë‚˜ì”© ì ‘ê·¼
    try :
        driver.get(codi_url)
        driver.implicitly_wait(3)
    except :
        print("ì´ ì—ëŸ¬ê°€ ë°œìƒí•˜ë©´ ë‹¤ìŒ ì½”ë””ë¶€í„° ë”°ë¡œ í¬ë¡¤ë§ í•´ì£¼ì‹œê¸¸ ë°”ëë‹ˆë‹¤!")
        continue
    
    # ì½”ë”” ì•ˆì— ìˆëŠ” ì•„ì´í…œì— ëŒ€í•œ element ë°›ì•„ì˜¤ê¸°
    item_list = driver.find_elements(By.CSS_SELECTOR, 'div.styling_list > div.swiper-slide')
    item_urls = []
    
    # ê° ì•„ì´í…œë“¤ì˜ url ì¶”ì¶œ
    for item in item_list:
        item_url = item.find_element(By.CSS_SELECTOR, "a.brand_item").get_attribute('href')
        item_urls.append(item_url)

    # ê° ì•„ì´í…œë“¤ì„ ìˆœíšŒí•˜ë©´ì„œ í¬ë¡¤ë§ ì§„í–‰
    for item_url in item_urls:
        try : 
            driver.get(item_url)
            driver.implicitly_wait(0.5) #í˜ì´ì§€ë¥¼ ë¡œë”©í•˜ëŠ” ì‹œê°„ë™ì•ˆ ëŒ€ê¸°
        except :
            continue
        
        print(f"Crawling item : {item_url}")
        item_info = EasyDict()
        item_info.item_url = item_url
        item_info.codi_id  = codi_id


        item_info.id            = get_item_id(item_url)
        item_info.name          = get_item_name(driver)

        category      = driver.find_elements(By.CSS_SELECTOR, "p.item_categories > a")
        item_info.big_class     = get_big_class(category)
        item_info.mid_class     = get_mid_class(category)

        product_info  = driver.find_elements(By.CSS_SELECTOR, "ul.product_article > li > p.product_article_contents > strong")
        item_info.brand         = get_brand(product_info)
        item_info.serial_number = get_serial_number(product_info)
        item_info.season        = get_season(driver, product_info)
        item_info.gender        = get_gender(driver)
        item_info.view          = get_view(driver)
        item_info.cum_sale      = get_cum_sale(driver)
        item_info.likes         = get_likes(driver)
        item_info.rating        = get_rating(driver)  
        item_info.price         = get_price(driver)
        item_info.img_url       = get_img_url(driver)
        
        #-- ì£¼ì˜: í¬ë¡¤ë§ì˜ ì¼ê´€ì„±ì´ ë†’ì§€ ì•ŠìŒ
        try: menu = driver.find_elements(By.CSS_SELECTOR, "div#goods_opt_area > select")
        except: menu = None
        item_info.color_list    = get_color(menu)
        item_info.size_list     = get_size(menu)

        item_info.tags_list       = get_tags_list(driver)
        item_info.buy_age_list    = get_buy_age_list(driver)
        item_info.buy_gender_list = get_buy_gender_list(driver)
        item_info.four_season_list, item_info.fit_list = get_fs_and_fit(driver)      
        
        # ìœ„ì—ì„œ í¬ë¡¤ë§í•œ ì •ë³´ë¥¼ sheetì— append
        save_to_sheets(sheets, item_info)

        # í˜„ì¬ ì•„ì´í…œ crawling ê²°ê³¼ ì¶œë ¥
        if _VERBOSE:
            print_crawled_item_info(item_info)

    cnt += 1

# í¬ë¡¤ë§ ê²°ê³¼ íŒŒì¼ë¡œ ì €ì¥
save_workbooks(workbooks)
driver.close()
